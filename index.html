<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SOTA Paper Recommendation</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        .maintainers {
            display: block;
            justify-content: center;
            gap: 30px; /* 增加间距 */
            padding: 20px;
            margin: 20px auto; /* 居中并增加外边距 */
            max-width: 800px; /* 限制最大宽度 */
        }
        
        .maintainer {
            text-align: center;
            transition: transform 0.3s ease, box-shadow 0.3s ease; /* 添加过渡效果 */
        }
        
        .maintainer:hover {
            transform: translateY(-5px); /* 悬停时上移 */
        }
        
        .maintainer img {
            width: 120px; /* 增加图片大小 */
            height: 120px;
            border-radius: 50%;
            object-fit: cover;
            border: 3px solid #007bff; /* 添加边框 */
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2); /* 图片阴影 */
        }
        
        .maintainer a {
            text-decoration: none; /* 去掉链接下划线 */
            color: inherit; /* 继承文字颜色 */
        }
        
        .maintainer a:hover {
            color: #007bff; /* 悬停时改变文字颜色 */
        }
        
        /* 高亮样式 */
        .date-link.active {
            background-color: #007bff; /* 蓝色背景 */
            color: white; /* 白色文字 */
            padding: 5px 10px; /* 内边距 */
            border-radius: 5px; /* 圆角 */
        }

        /* 默认样式 */
        .date-link {
            text-decoration: none; /* 去掉下划线 */
            color: #333; /* 默认文字颜色 */
            padding: 5px 10px; /* 内边距 */
            display: inline-block; /* 使链接可以设置宽度和高度 */
        }

        /* 悬停效果 */
        .date-link:hover {
            background-color: #f0f0f0; /* 浅灰色背景 */
            color: #007bff; /* 蓝色文字 */
        }

        /* 论文列表样式 */
        .paper-item {
            margin-bottom: 20px;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }

        .paper-item.hidden {
            display: none; /* 隐藏不符合条件的论文 */
        }

        .paper-thumbnail {
            max-width: 100%; /* 图片最大宽度为容器宽度 */
            height: auto; /* 高度自适应，保持比例 */
            width: 600px; /* 设置图片宽度 */
            border-radius: 5px; /* 圆角 */
            margin-bottom: 10px; /* 图片与下方内容的间距 */
        }
        
    </style>
</head>
<body>
    <div class="container">
        <!-- Sidebar for Date Selection -->
        <div class="sidebar">
            <h3>Filter by Month</h3>
            <ul id="date-filter">
                <li><a href="#" data-date="all" class="date-link">All</a></li>
                <li><a href="#" data-date="2025-01" class="date-link">01-2025</a></li>
                <li><a href="#" data-date="2024-12" class="date-link">12-2024</a></li>
            </ul>
        </div>

        <!-- Main Content -->
        <div class="main-content">
            <!-- Star Button -->
            <div class="star-button">
                <a href="https://github.com/WayneJin0918/SOTA-paper-rating.io?star=1" target="_blank">
                    ⭐ Star this Repository
                </a>
            </div>
            <h1>SOTA Paper Recommendation</h1>
            <div class="paper-list">


                <!-- Paper 19 -->
                <div class="paper-item" data-paper-id="18" data-date="2025-01-17">
                    <img src="images/showo_cot.png" alt="Paper 19 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">Can We Generate Images with CoT? Let’s Verify and Reinforce Image Generation Step by Step</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★☆☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>Purshow Review</h3>
                        <p> A first exploration of cot image generation in "mask AR". Some problems are worth further exploration: 
1. Adapt the method to more models, even normal AR models instead of mask AR, 2. Suitable benchmarks that truly test the model's cot benefits 3. Potential for unifying with AR understanding COT <p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.13926" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 19 --> 

                
                <!-- Paper 18 -->
                <div class="paper-item" data-paper-id="18" data-date="2025-01-17">
                    <img src="images/ViTok.png" alt="Paper 18 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">Learnings from Scaling Visual Tokenizers for Reconstruction and Generation</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>Purshow Review</h3>
                        <p> An exploration of scaling in auto-encoders.<p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.09755" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 18 --> 
                
                <!-- Paper 17 -->
                <div class="paper-item" data-paper-id="17" data-date="2025-01-15">
                    <img src="images/TA-TiTok-01-15.png" alt="Paper 17 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>Integrates textual information during the tokenizer decoding stage (i.e., de-tokenization), accelerating convergence and enhancing performance.<p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.07730" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 17 --> 

                <!-- Paper 16 -->
                <div class="paper-item" data-paper-id="16" data-date="2025-01-15">
                    <img src="images/DITGAN-01-15.png" alt="Paper 16 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">Diffusion Adversarial Post-Training for One-Step Video Generation</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>DiT as GAN achieves one-step generation in video.<p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.08316" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 16 --> 

                <!-- Paper 15 -->
                <div class="paper-item" data-paper-id="15" data-date="2025-01-15">
                    <img src="images/Game-01-15.png" alt="Paper 15 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">GameFactory: Creating New Games with Generative Interactive Videos</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>By learning motion controls from a small-scale first-person Minecraft dataset, this framework can transfer these control capabilities to open-domain videos.<p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.08325" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 15 --> 

                <!-- Paper 14 -->
                <div class="paper-item" data-paper-id="14" data-date="2025-01-14">
                    <img src="images/LEO-01-14.png" alt="Paper 14 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">LEO: Boosting Mixture of Vision Encoders for Multimodal Large Language Models</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★☆☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>Feature fusion in MLLMs for MoE vision encoder, should be scaling to make stonger conclusions.<p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.06986" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 14 -->                  

                <!-- Paper 13 -->
                <div class="paper-item" data-paper-id="13" data-date="2025-01-10">
                    <img src="images/EVAE-01-07.png" alt="Paper 13 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">An Empirical Study of Autoregressive Pre-training from Videos</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>Pretrain on video using generative method, although performance is not good enough, but it's a good try. <p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.05453" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 13 -->  

                <!-- Paper 12 -->
                <div class="paper-item" data-paper-id="12" data-date="2025-01-07">
                    <img src="images/GSMAE-01-07.png" alt="Paper 12 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">Gaussian Masked Autoencoders</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>3D MAE for GS, very solid meta style.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.03229" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 12 -->   

                <!-- Paper 11 -->
                <div class="paper-item" data-paper-id="11" data-date="2025-01-01">
                    <img src="images/clusterAR-01-01.png" alt="Paper 11 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★☆☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>I think semantic-level token clustering is a good attempt in AR image generation. LCM has tried sentence-level generation in AR language generation. Perhaps instead of simple clustering, people can try to do something similar in image?</p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.00880" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 11 -->                

                <!-- Paper 10 -->
                <div class="paper-item" data-paper-id="10" data-date="2025-01-01">
                    <img src="images/DDit-01-01.png" alt="Paper 10 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">Dual Diffusion for Unified Image Generation and Understanding</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>Unified understanding and generative model but using generative model (DiTs) as backbone.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.00289" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 10 -->                  

                <!-- Paper 9 -->
                <div class="paper-item" data-paper-id="9" data-date="2025-01-01">
                    <img src="images/MLLMas-01-01.png" alt="Paper 9 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">MLLM-as-a-Judge for Image Safety without Human Labeling</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>LLM-as-judge is already very common in the field of language models. There was a version of MLLMs before, but not solid enough. I think the bar for this kind of research is very low, but it is difficult to dig deeper. This paper is able to bring up some interesting points such as context bias in images.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2501.00192" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 9 -->                

                <!-- Paper 8 -->
                <div class="paper-item" data-paper-id="8" data-date="2024-12-31">
                    <img src="images/overthink-12-30.png" alt="Paper 8 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">Do NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMs</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆ (for findings)</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>Good findings, bad methods.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2412.21187" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 8 -->

                <!-- Paper 7 -->
                <div class="paper-item" data-paper-id="7" data-date="2024-12-25">
                    <img src="images/flux-12-25.png" alt="Paper 7 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">1.58-bit FLUX</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆ (if open)</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>MLsys is important! But the details are not enough to comment and not open source.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2412.18653" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 7 -->

                <!-- Paper 6 -->
                <div class="paper-item" data-paper-id="6" data-date="2024-12-25">
                    <img src="images/dific-12-30.png" alt="Paper 6 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">DiFiC: Your Diffusion Model Holds the Secret to Fine-Grained Clustering</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★☆☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>Using the potential discrimination ability of DMs for fine-grained clustering.I think this work is worth expanding, especially since there are similar papers that explore the classification ability of DMs (although it is a coarse classification). It also further illustrates the potential of using generative models for representation learning.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2412.18838" class="code-link" target="_blank">Paper Link</a>
                </div> 
                <!-- Paper 6 -->

                <!-- Paper 5 -->
                <div class="paper-item" data-paper-id="5" data-date="2024-12-27">
                    <img src="images/gen-12-30.png" alt="Paper 5 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">Generative Video Propagation</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★☆☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>A magical framework that uses artificial data to promote related tasks. A strong counterexample constraint is given to be the learning objective. I think there is a certain potential to use it in motion or more implicit control references.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2412.19761" class="code-link" target="_blank">Paper Link</a>
                </div>
                <!-- Paper 5 -->

                <!-- Paper 4 -->
                <div class="paper-item" data-paper-id="4" data-date="2024-12-20">
                    <img src="images/clear_12-20.png" alt="Paper 4 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★☆☆☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>The locality improvement based on conv is definitely helpful to transformer, and the introduction into DiTs is also an inevitable trend. This paper is worth learning from because it is well written (clear motivation and method), and the code maintenance is also worth learning.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2412.16112" class="code-link" target="_blank">Paper Link</a>
                </div>
                <!-- Paper 4 -->
                
                <!-- Paper 3 -->
                <div class="paper-item" data-paper-id="3" data-date="2024-12-20">
                    <img src="images/meta_12-26.png" alt="Paper 3 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">MetaMorph: Multimodal Understanding and Generation via Instruction Tuning</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>This paper reveals how generation and understanding can be empirically proven to promote each other. At the same time, I noticed that the features of VIT encoder are very powerful and our existing fine-tuning methods have not fully utilized them.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2412.14164" class="code-link" target="_blank">Paper Link</a>
                </div>
                <!-- Paper 3 -->
                
                <!-- Paper 2 -->
                <div class="paper-item" data-paper-id="2" data-date="2024-12-26">
                    <img src="images/dino_24-12.png" alt="Paper 2 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★★★★☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>Simple but effective. The combination of language supervised and unsupervised SSL. I think it is good idea to test on <a href="https://github.com/tsb0601/MMVP">MMVP</a>.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2412.16334" class="code-link" target="_blank">Paper Link</a>
                </div>
                <!-- Paper 2 -->

                <!-- Paper 1 -->
                <div class="paper-item" data-paper-id="1" data-date="2024-12-26">
                    <img src="images/vla_bench_24-12.jpg" alt="Paper 1 Thumbnail" class="paper-thumbnail">
                    <h2 class="paper-title">VLABench: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks</h2>
                    <div class="paper-rating">
                        <span>Rating: </span>
                        <span class="stars">★☆☆☆☆</span>
                    </div>
                    <div class="paper-review">
                        <h3>SOTA Review</h3>
                        <p>General benchmark for vla, not really impressive, but can be referenced in the future design.</p>
                    </div>
                    <a href="https://arxiv.org/abs/2412.18194" class="code-link" target="_blank">Paper Link</a>
                </div>
                <!-- Paper 1 -->


                
            </div>
        </div>
    </div>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            const dateFilterLinks = document.querySelectorAll('#date-filter a');
            const paperItems = document.querySelectorAll('.paper-item');

            // 从 localStorage 中获取选中的日期
            const selectedDate = localStorage.getItem('selectedDate') || 'all';

            // 设置初始高亮状态
            dateFilterLinks.forEach(link => {
                if (link.getAttribute('data-date') === selectedDate) {
                    link.classList.add('active');
                } else {
                    link.classList.remove('active');
                }
            });

            // 过滤论文列表
            paperItems.forEach(item => {
                const itemDate = item.getAttribute('data-date');
                const itemYearMonth = itemDate.slice(0, 7); // Extract YYYY-MM

                if (selectedDate === 'all' || selectedDate === itemYearMonth) {
                    item.style.display = 'block'; // Show the item
                } else {
                    item.style.display = 'none'; // Hide the item
                }
            });

            // 监听日期过滤选项的点击事件
            dateFilterLinks.forEach(link => {
                link.addEventListener('click', function (e) {
                    e.preventDefault();
                    const selectedDate = this.getAttribute('data-date');

                    // 保存选中的日期到 localStorage
                    localStorage.setItem('selectedDate', selectedDate);

                    // 移除所有选项的高亮样式
                    dateFilterLinks.forEach(link => link.classList.remove('active'));

                    // 为当前点击的选项添加高亮样式
                    this.classList.add('active');

                    // 过滤论文列表
                    paperItems.forEach(item => {
                        const itemDate = item.getAttribute('data-date');
                        const itemYearMonth = itemDate.slice(0, 7); // Extract YYYY-MM

                        if (selectedDate === 'all' || selectedDate === itemYearMonth) {
                            item.style.display = 'block'; // Show the item
                        } else {
                            item.style.display = 'none'; // Hide the item
                        }
                    });
                });
            });
        });
    </script>

    <section class="section" id="Contribution">
      <div class="container content">
        <div class="maintainers">
          <div class="maintainer">
            <a href="https://github.com/WayneJin0918">
              <img src="images/137654456.jpg" style="width: 100px; height: 100px; border-radius: 50%; object-fit: cover;" alt="Weiyang Jin">
            </a>
          </div>
          <div class="maintainer">
            <a href="https://github.com/Weiraner">
              <img src="images/160975102.jpg" style="width: 100px; height: 100px; border-radius: 50%; object-fit: cover;" alt="Shiya Su">
            </a>
          </div>
        </div>
      </div>
    </section>
    
</body>
</html>
